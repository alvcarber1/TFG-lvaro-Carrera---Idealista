{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta celda importamos todas las bibliotecas necesarias para el análisis y modelado. Incluimos bibliotecas para manipulación de datos como pandas y numpy, herramientas de machine learning como scikit-learn, xgboost y otras utilidades como matplotlib para visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos tres conjuntos de datos distintos que contienen información sobre inmuebles en Madrid. Cada archivo contiene datos específicos (como ubicación, características generales y detalles del precio).\n",
    "\n",
    "Eliminamos columnas innecesarias. Es común que al guardar y cargar archivos CSV se cree una columna adicional (Unnamed: 0) que no aporta información útil. Además, se eliminan columnas duplicadas de latitud y longitud de uno de los datasets.\n",
    "\n",
    "Combinamos los tres conjuntos de datos utilizando id como clave común. Eliminamos las columnas duplicadas creadas durante el proceso de unión y guardamos el dataset unificado en un nuevo archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas finales: ['id', 'latitude', 'longitude', 'address', 'sq_mt_built', 'n_rooms', 'n_bathrooms', 'n_floors', 'sq_mt_allotment', 'floor', 'buy_price', 'is_renewal_needed', 'has_lift', 'is_exterior', 'energy_certificate', 'has_parking', 'neighborhood', 'district', 'house_type', 'Unnamed: 0', 'title', 'subtitle', 'sq_mt_useful', 'raw_address', 'is_exact_address_hidden', 'street_name', 'street_number', 'portal', 'is_floor_under', 'door', 'neighborhood_id', 'operation', 'rent_price', 'rent_price_by_area', 'is_rent_price_known', 'buy_price_by_area', 'is_buy_price_known', 'house_type_id', 'is_new_development', 'built_year', 'has_central_heating', 'has_individual_heating', 'are_pets_allowed', 'has_ac', 'has_fitted_wardrobes', 'has_garden', 'has_pool', 'has_terrace', 'has_balcony', 'has_storage_room', 'is_furnished', 'is_kitchen_equipped', 'is_accessible', 'has_green_zones', 'has_private_parking', 'has_public_parking', 'is_parking_included_in_price', 'parking_price', 'is_orientation_north', 'is_orientation_west', 'is_orientation_south', 'is_orientation_east']\n",
      "Número de filas: 6735\n"
     ]
    }
   ],
   "source": [
    "lat_lon_df = pd.read_csv('../data/lat_lon_houses_Madrid_cleaned.csv')\n",
    "houses_clean_df = pd.read_csv('../data/madrid_houses_clean.csv')\n",
    "houses_df = pd.read_csv('../data/houses_Madrid.csv')\n",
    "\n",
    "if 'Unnamed: 0' in houses_clean_df.columns:\n",
    "    houses_clean_df = houses_clean_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Para latitude y longitude, usaremos solo los datos de lat_lon_df\n",
    "# Eliminamos estas columnas de los otros datasets\n",
    "if 'latitude' in houses_df.columns:\n",
    "    houses_df = houses_df.drop(['latitude', 'longitude'], axis=1)\n",
    "\n",
    "merged_df = pd.merge(lat_lon_df, houses_clean_df, on='id', how='inner')\n",
    "\n",
    "# Luego unimos con houses_df, si hay columnas con el mismo nombre, se le añade como sufijo 'drop' \n",
    "final_df = pd.merge(merged_df, houses_df, on='id', how='left', suffixes=('', '_drop'))\n",
    "\n",
    "final_df = final_df.loc[:, ~final_df.columns.str.endswith('_drop')]\n",
    "\n",
    "print(\"Columnas finales:\", final_df.columns.tolist())\n",
    "print(\"Número de filas:\", len(final_df))\n",
    "\n",
    "final_df.to_csv('../data/unified_houses_madrid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset unificado para revisar las primeras filas y el tipo de datos en cada columna, asegurándonos de que la unión y el guardado se hicieron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id   latitude  longitude  \\\n",
      "0  21742  40.344512  -3.689441   \n",
      "1  21741  40.353850  -3.698362   \n",
      "2  21738  40.353380  -3.690115   \n",
      "3  21733  40.353235  -3.689915   \n",
      "4  21730  40.361389  -3.689422   \n",
      "\n",
      "                                             address  sq_mt_built  n_rooms  \\\n",
      "0        Calle de Godella, 64, San Cristóbal, Madrid         64.0        2   \n",
      "1  Calle de la del Manojo de Rosas, Los Ángeles, ...         70.0        3   \n",
      "2  Carretera de Villaverde a Vallecas, Los Rosale...        108.0        2   \n",
      "3       Calle de Martinez Oviol, Los Rosales, Madrid         85.0        2   \n",
      "4    Calle de la Unanimidad, 67, Los Rosales, Madrid        123.0        3   \n",
      "\n",
      "   n_bathrooms  n_floors  sq_mt_allotment  floor  ...  is_accessible  \\\n",
      "0            1         1              0.0      3  ...            NaN   \n",
      "1            1         1              0.0      4  ...            NaN   \n",
      "2            2         1              0.0      4  ...            NaN   \n",
      "3            1         1              0.0      7  ...            NaN   \n",
      "4            2         1              0.0      4  ...            NaN   \n",
      "\n",
      "   has_green_zones  has_private_parking  has_public_parking  \\\n",
      "0              NaN                  NaN                 NaN   \n",
      "1              NaN                  NaN                 NaN   \n",
      "2             True                  NaN                 NaN   \n",
      "3              NaN                  NaN                 NaN   \n",
      "4              NaN                  NaN                 NaN   \n",
      "\n",
      "   is_parking_included_in_price  parking_price  is_orientation_north  \\\n",
      "0                           NaN            NaN                 False   \n",
      "1                           NaN            NaN                   NaN   \n",
      "2                          True            0.0                  True   \n",
      "3                           NaN            NaN                 False   \n",
      "4                           NaN            NaN                 False   \n",
      "\n",
      "   is_orientation_west  is_orientation_south  is_orientation_east  \n",
      "0                 True                 False                False  \n",
      "1                  NaN                   NaN                  NaN  \n",
      "2                 True                  True                 True  \n",
      "3                 True                 False                False  \n",
      "4                False                 False                 True  \n",
      "\n",
      "[5 rows x 62 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6735 entries, 0 to 6734\n",
      "Data columns (total 62 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   id                            6735 non-null   int64  \n",
      " 1   latitude                      6735 non-null   float64\n",
      " 2   longitude                     6735 non-null   float64\n",
      " 3   address                       6735 non-null   object \n",
      " 4   sq_mt_built                   6735 non-null   float64\n",
      " 5   n_rooms                       6735 non-null   int64  \n",
      " 6   n_bathrooms                   6735 non-null   int64  \n",
      " 7   n_floors                      6735 non-null   int64  \n",
      " 8   sq_mt_allotment               6735 non-null   float64\n",
      " 9   floor                         6735 non-null   int64  \n",
      " 10  buy_price                     6735 non-null   int64  \n",
      " 11  is_renewal_needed             6735 non-null   bool   \n",
      " 12  has_lift                      6735 non-null   bool   \n",
      " 13  is_exterior                   6735 non-null   bool   \n",
      " 14  energy_certificate            6735 non-null   int64  \n",
      " 15  has_parking                   6735 non-null   bool   \n",
      " 16  neighborhood                  6735 non-null   int64  \n",
      " 17  district                      6735 non-null   int64  \n",
      " 18  house_type                    6735 non-null   int64  \n",
      " 19  Unnamed: 0                    6735 non-null   int64  \n",
      " 20  title                         6735 non-null   object \n",
      " 21  subtitle                      6735 non-null   object \n",
      " 22  sq_mt_useful                  3077 non-null   float64\n",
      " 23  raw_address                   6735 non-null   object \n",
      " 24  is_exact_address_hidden       6735 non-null   bool   \n",
      " 25  street_name                   6636 non-null   object \n",
      " 26  street_number                 3342 non-null   object \n",
      " 27  portal                        0 non-null      float64\n",
      " 28  is_floor_under                6578 non-null   object \n",
      " 29  door                          0 non-null      float64\n",
      " 30  neighborhood_id               6735 non-null   object \n",
      " 31  operation                     6735 non-null   object \n",
      " 32  rent_price                    6735 non-null   int64  \n",
      " 33  rent_price_by_area            0 non-null      float64\n",
      " 34  is_rent_price_known           6735 non-null   bool   \n",
      " 35  buy_price_by_area             6735 non-null   int64  \n",
      " 36  is_buy_price_known            6735 non-null   bool   \n",
      " 37  house_type_id                 6567 non-null   object \n",
      " 38  is_new_development            6505 non-null   object \n",
      " 39  built_year                    3246 non-null   float64\n",
      " 40  has_central_heating           4507 non-null   object \n",
      " 41  has_individual_heating        4507 non-null   object \n",
      " 42  are_pets_allowed              0 non-null      float64\n",
      " 43  has_ac                        3463 non-null   object \n",
      " 44  has_fitted_wardrobes          4374 non-null   object \n",
      " 45  has_garden                    201 non-null    object \n",
      " 46  has_pool                      1159 non-null   object \n",
      " 47  has_terrace                   2846 non-null   object \n",
      " 48  has_balcony                   1197 non-null   object \n",
      " 49  has_storage_room              2056 non-null   object \n",
      " 50  is_furnished                  0 non-null      float64\n",
      " 51  is_kitchen_equipped           0 non-null      float64\n",
      " 52  is_accessible                 1489 non-null   object \n",
      " 53  has_green_zones               1331 non-null   object \n",
      " 54  has_private_parking           0 non-null      float64\n",
      " 55  has_public_parking            0 non-null      float64\n",
      " 56  is_parking_included_in_price  1879 non-null   object \n",
      " 57  parking_price                 1879 non-null   float64\n",
      " 58  is_orientation_north          3834 non-null   object \n",
      " 59  is_orientation_west           3834 non-null   object \n",
      " 60  is_orientation_south          3834 non-null   object \n",
      " 61  is_orientation_east           3834 non-null   object \n",
      "dtypes: bool(7), float64(15), int64(13), object(27)\n",
      "memory usage: 2.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/unified_houses_madrid.csv')\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos las variables predictoras (X) y la variable objetivo (y). También revisamos si existen valores nulos en las variables predictoras para manejarlos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas con valores nulos:\n",
      "sq_mt_useful                    3658\n",
      "street_name                       99\n",
      "street_number                   3393\n",
      "portal                          6735\n",
      "is_floor_under                   157\n",
      "door                            6735\n",
      "rent_price_by_area              6735\n",
      "house_type_id                    168\n",
      "is_new_development               230\n",
      "built_year                      3489\n",
      "has_central_heating             2228\n",
      "has_individual_heating          2228\n",
      "are_pets_allowed                6735\n",
      "has_ac                          3272\n",
      "has_fitted_wardrobes            2361\n",
      "has_garden                      6534\n",
      "has_pool                        5576\n",
      "has_terrace                     3889\n",
      "has_balcony                     5538\n",
      "has_storage_room                4679\n",
      "is_furnished                    6735\n",
      "is_kitchen_equipped             6735\n",
      "is_accessible                   5246\n",
      "has_green_zones                 5404\n",
      "has_private_parking             6735\n",
      "has_public_parking              6735\n",
      "is_parking_included_in_price    4856\n",
      "parking_price                   4856\n",
      "is_orientation_north            2901\n",
      "is_orientation_west             2901\n",
      "is_orientation_south            2901\n",
      "is_orientation_east             2901\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../data/unified_houses_madrid.csv')\n",
    "\n",
    "ids = df['id'].copy()\n",
    "\n",
    "\n",
    "X = df.drop(['buy_price'], axis=1)\n",
    "y = df['buy_price']\n",
    "\n",
    "X_train = X.drop(['id'], axis=1)\n",
    "\n",
    "print(\"Columnas con valores nulos:\")\n",
    "print(X.isnull().sum()[X.isnull().sum() > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificamos y clasificamos las columnas según el tipo de información que contienen: numéricas, categóricas y binarias. Esto es crucial para definir estrategias de preprocesamiento específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles en el DataFrame:\n",
      "['id', 'latitude', 'longitude', 'address', 'sq_mt_built', 'n_rooms', 'n_bathrooms', 'n_floors', 'sq_mt_allotment', 'floor', 'buy_price', 'is_renewal_needed', 'has_lift', 'is_exterior', 'energy_certificate', 'has_parking', 'neighborhood', 'district', 'house_type', 'Unnamed: 0', 'title', 'subtitle', 'sq_mt_useful', 'raw_address', 'is_exact_address_hidden', 'street_name', 'street_number', 'portal', 'is_floor_under', 'door', 'neighborhood_id', 'operation', 'rent_price', 'rent_price_by_area', 'is_rent_price_known', 'buy_price_by_area', 'is_buy_price_known', 'house_type_id', 'is_new_development', 'built_year', 'has_central_heating', 'has_individual_heating', 'are_pets_allowed', 'has_ac', 'has_fitted_wardrobes', 'has_garden', 'has_pool', 'has_terrace', 'has_balcony', 'has_storage_room', 'is_furnished', 'is_kitchen_equipped', 'is_accessible', 'has_green_zones', 'has_private_parking', 'has_public_parking', 'is_parking_included_in_price', 'parking_price', 'is_orientation_north', 'is_orientation_west', 'is_orientation_south', 'is_orientation_east']\n",
      "\n",
      "Verificando existencia de columnas:\n",
      "sq_mt_built: ✓\n",
      "sq_mt_useful: ✓\n",
      "n_rooms: ✓\n",
      "n_bathrooms: ✓\n",
      "floor: ✓\n",
      "built_year: ✓\n",
      "buy_price_by_area: ✓\n",
      "latitude: ✓\n",
      "longitude: ✓\n",
      "house_type: ✓\n",
      "energy_certificate: ✓\n",
      "district: ✓\n",
      "neighborhood: ✓\n",
      "has_lift: ✓\n",
      "is_exterior: ✓\n",
      "has_parking: ✓\n",
      "is_new_development: ✓\n",
      "has_central_heating: ✓\n",
      "has_individual_heating: ✓\n",
      "has_ac: ✓\n",
      "has_garden: ✓\n",
      "has_pool: ✓\n",
      "has_terrace: ✓\n",
      "has_storage_room: ✓\n",
      "is_furnished: ✓\n",
      "is_orientation_north: ✓\n",
      "is_orientation_south: ✓\n",
      "is_orientation_east: ✓\n",
      "is_orientation_west: ✓\n",
      "\n",
      "Valores nulos por columna:\n",
      "sq_mt_built                  0\n",
      "sq_mt_useful              3658\n",
      "n_rooms                      0\n",
      "n_bathrooms                  0\n",
      "floor                        0\n",
      "built_year                3489\n",
      "buy_price_by_area            0\n",
      "latitude                     0\n",
      "longitude                    0\n",
      "house_type                   0\n",
      "energy_certificate           0\n",
      "district                     0\n",
      "neighborhood                 0\n",
      "has_lift                     0\n",
      "is_exterior                  0\n",
      "has_parking                  0\n",
      "is_new_development         230\n",
      "has_central_heating       2228\n",
      "has_individual_heating    2228\n",
      "has_ac                    3272\n",
      "has_garden                6534\n",
      "has_pool                  5576\n",
      "has_terrace               3889\n",
      "has_storage_room          4679\n",
      "is_furnished              6735\n",
      "is_orientation_north      2901\n",
      "is_orientation_south      2901\n",
      "is_orientation_east       2901\n",
      "is_orientation_west       2901\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Primero, veamos qué columnas tenemos en el DataFrame\n",
    "print(\"Columnas disponibles en el DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# 2. Ajustamos las listas de columnas según las que realmente existen\n",
    "columnas_numericas = [\n",
    "    'sq_mt_built', 'sq_mt_useful', 'n_rooms', 'n_bathrooms', \n",
    "    'floor', 'built_year', 'buy_price_by_area',\n",
    "    'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "columnas_categoricas = [\n",
    "    'house_type', 'energy_certificate', 'district', 'neighborhood'\n",
    "]\n",
    "\n",
    "columnas_binarias = [\n",
    "    'has_lift', 'is_exterior', 'has_parking', 'is_new_development',\n",
    "    'has_central_heating', 'has_individual_heating', 'has_ac',\n",
    "    'has_garden', 'has_pool', 'has_terrace', 'has_storage_room',\n",
    "    'is_furnished', 'is_orientation_north', 'is_orientation_south',\n",
    "    'is_orientation_east', 'is_orientation_west'\n",
    "]\n",
    "\n",
    "# 3. Verificar que todas las columnas existen\n",
    "print(\"\\nVerificando existencia de columnas:\")\n",
    "for col in columnas_numericas + columnas_categoricas + columnas_binarias:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: ✓\")\n",
    "    else:\n",
    "        print(f\"{col}: ✗\")\n",
    "\n",
    "# 4. Preparar X e y\n",
    "X = df[columnas_numericas + columnas_categoricas + columnas_binarias]\n",
    "y = df['buy_price']\n",
    "\n",
    "# 5. Verificar valores nulos antes de la transformación\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables binarias se transforman en valores numéricos (1.0 y 0.0) utilizando numpy. A continuación, se manejan los valores nulos en las variables numéricas y categóricas, aplicando distintas estrategias de imputación según el tipo de variable: para variables numéricas, se rellenan con la mediana, media o moda según corresponda, y en el caso de las variables categóricas, se rellenan con la moda de cada barrio o un valor predeterminado para columnas específicas como energy_certificate.\n",
    "\n",
    "Después, se asegura que las columnas tengan los tipos de datos correctos (float64 para variables numéricas y string para las categóricas). El código también verifica si aún existen valores nulos, los tipos de datos y la distribución de las variables binarias.\n",
    "\n",
    "Finalmente, se comprueba que los valores en las variables binarias estén dentro del rango esperado (0 y 1). Todo esto garantiza que los datos estén listos para ser utilizados en un modelo de machine learning, con valores limpios y en el formato adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos después del procesamiento:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Tipos de datos después del procesamiento:\n",
      "sq_mt_built                      float64\n",
      "sq_mt_useful                     float64\n",
      "n_rooms                          float64\n",
      "n_bathrooms                      float64\n",
      "floor                            float64\n",
      "built_year                       float64\n",
      "buy_price_by_area                float64\n",
      "latitude                         float64\n",
      "longitude                        float64\n",
      "house_type                string[python]\n",
      "energy_certificate        string[python]\n",
      "district                  string[python]\n",
      "neighborhood              string[python]\n",
      "has_lift                         float64\n",
      "is_exterior                      float64\n",
      "has_parking                      float64\n",
      "is_new_development               float64\n",
      "has_central_heating              float64\n",
      "has_individual_heating           float64\n",
      "has_ac                           float64\n",
      "has_garden                       float64\n",
      "has_pool                         float64\n",
      "has_terrace                      float64\n",
      "has_storage_room                 float64\n",
      "is_furnished                     float64\n",
      "is_orientation_north             float64\n",
      "is_orientation_south             float64\n",
      "is_orientation_east              float64\n",
      "is_orientation_west              float64\n",
      "dtype: object\n",
      "\n",
      "Distribución de variables binarias:\n",
      "\n",
      "has_lift:\n",
      "0.0: 2210 (32.8%)\n",
      "1.0: 4525 (67.2%)\n",
      "\n",
      "is_exterior:\n",
      "0.0: 711 (10.6%)\n",
      "1.0: 6024 (89.4%)\n",
      "\n",
      "has_parking:\n",
      "0.0: 4856 (72.1%)\n",
      "1.0: 1879 (27.9%)\n",
      "\n",
      "is_new_development:\n",
      "0.0: 6626 (98.4%)\n",
      "1.0: 109 (1.6%)\n",
      "\n",
      "has_central_heating:\n",
      "0.0: 5543 (82.3%)\n",
      "1.0: 1192 (17.7%)\n",
      "\n",
      "has_individual_heating:\n",
      "0.0: 3420 (50.8%)\n",
      "1.0: 3315 (49.2%)\n",
      "\n",
      "has_ac:\n",
      "0.0: 3272 (48.6%)\n",
      "1.0: 3463 (51.4%)\n",
      "\n",
      "has_garden:\n",
      "0.0: 6534 (97.0%)\n",
      "1.0: 201 (3.0%)\n",
      "\n",
      "has_pool:\n",
      "0.0: 5576 (82.8%)\n",
      "1.0: 1159 (17.2%)\n",
      "\n",
      "has_terrace:\n",
      "0.0: 3889 (57.7%)\n",
      "1.0: 2846 (42.3%)\n",
      "\n",
      "has_storage_room:\n",
      "0.0: 4679 (69.5%)\n",
      "1.0: 2056 (30.5%)\n",
      "\n",
      "is_furnished:\n",
      "0.0: 6735 (100.0%)\n",
      "\n",
      "is_orientation_north:\n",
      "0.0: 5829 (86.5%)\n",
      "1.0: 906 (13.5%)\n",
      "\n",
      "is_orientation_south:\n",
      "0.0: 4853 (72.1%)\n",
      "1.0: 1882 (27.9%)\n",
      "\n",
      "is_orientation_east:\n",
      "0.0: 5075 (75.4%)\n",
      "1.0: 1660 (24.6%)\n",
      "\n",
      "is_orientation_west:\n",
      "0.0: 5426 (80.6%)\n",
      "1.0: 1309 (19.4%)\n",
      "\n",
      "Verificación de rangos:\n",
      "has_lift: min=0.0, max=1.0\n",
      "is_exterior: min=0.0, max=1.0\n",
      "has_parking: min=0.0, max=1.0\n",
      "is_new_development: min=0.0, max=1.0\n",
      "has_central_heating: min=0.0, max=1.0\n",
      "has_individual_heating: min=0.0, max=1.0\n",
      "has_ac: min=0.0, max=1.0\n",
      "has_garden: min=0.0, max=1.0\n",
      "has_pool: min=0.0, max=1.0\n",
      "has_terrace: min=0.0, max=1.0\n",
      "has_storage_room: min=0.0, max=1.0\n",
      "is_furnished: min=0.0, max=0.0\n",
      "is_orientation_north: min=0.0, max=1.0\n",
      "is_orientation_south: min=0.0, max=1.0\n",
      "is_orientation_east: min=0.0, max=1.0\n",
      "is_orientation_west: min=0.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Crear una copia del DataFrame para evitar warnings\n",
    "X = df[columnas_numericas + columnas_categoricas + columnas_binarias].copy()\n",
    "y = df['buy_price'].copy()\n",
    "\n",
    "# 2. Convertir variables binarias usando numpy\n",
    "for col in columnas_binarias:\n",
    "    if col in X.columns:\n",
    "        # Convertir a array numpy, procesar y volver a pandas\n",
    "        valores = np.where(X[col].isin([True, 'True', '1', 1]), 1.0, 0.0)\n",
    "        X[col] = pd.Series(valores, index=X.index, dtype='float64')\n",
    "\n",
    "# 3. Manejo de valores nulos para variables numéricas\n",
    "for col in columnas_numericas:\n",
    "    if col in X.columns:\n",
    "        if col == 'built_year':\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "        elif col in ['buy_price_by_area', 'sq_mt_built', 'sq_mt_useful']:\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "        elif col in ['n_rooms', 'n_bathrooms', 'floor']:\n",
    "            X[col] = X[col].fillna(X[col].mode().iloc[0])\n",
    "        elif col in ['latitude', 'longitude']:\n",
    "            medianas_barrio = X.groupby('neighborhood')[col].transform('median')\n",
    "            X[col] = X[col].fillna(medianas_barrio)\n",
    "\n",
    "# 4. Manejo de valores nulos para variables categóricas\n",
    "for col in columnas_categoricas:\n",
    "    if col in X.columns:\n",
    "        if col == 'energy_certificate':\n",
    "            X[col] = X[col].fillna('NO_DISPONIBLE')\n",
    "        else:\n",
    "            modas_barrio = X.groupby('neighborhood')[col].transform(lambda x: x.mode().iloc[0])\n",
    "            X[col] = X[col].fillna(modas_barrio)\n",
    "\n",
    "# 5. Asegurar tipos de datos correctos\n",
    "X[columnas_numericas] = X[columnas_numericas].astype('float64')\n",
    "X[columnas_categoricas] = X[columnas_categoricas].astype('string')\n",
    "\n",
    "# 6. Verificar el resultado\n",
    "print(\"Valores nulos después del procesamiento:\")\n",
    "print(X.isnull().sum()[X.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nTipos de datos después del procesamiento:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "# 7. Verificar valores únicos en variables binarias\n",
    "print(\"\\nDistribución de variables binarias:\")\n",
    "for col in columnas_binarias:\n",
    "    counts = X[col].value_counts().sort_index()\n",
    "    total = len(X)\n",
    "    print(f\"\\n{col}:\")\n",
    "    for valor, cantidad in counts.items():\n",
    "        porcentaje = (cantidad/total) * 100\n",
    "        print(f\"{valor:.1f}: {cantidad} ({porcentaje:.1f}%)\")\n",
    "\n",
    "# 8. Verificar que los valores son correctos\n",
    "print(\"\\nVerificación de rangos:\")\n",
    "for col in columnas_binarias:\n",
    "    min_val = X[col].min()\n",
    "    max_val = X[col].max()\n",
    "    print(f\"{col}: min={min_val}, max={max_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código define un conjunto de transformadores para preprocesar diferentes tipos de variables. Las variables numéricas se estandarizan con StandardScaler, las categóricas se convierten en variables binarias mediante OneHotEncoder, y las binarias se transforman en valores numéricos (0.0 y 1.0) usando una función personalizada. Finalmente, se agrupan estos transformadores en un ColumnTransformer que aplica cada uno al tipo de columna correspondiente en el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Definir una función normal para la transformación binaria\n",
    "def convert_to_float(x):\n",
    "    return x.astype(float)\n",
    "\n",
    "# Numéricas: Estandarización\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categóricas: One-Hot Encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Variables binarias: Convertir a números\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('bool_to_int', FunctionTransformer(convert_to_float))\n",
    "])\n",
    "\n",
    "# Crear el preprocesador completo\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, columnas_numericas),\n",
    "        ('cat', categorical_transformer, columnas_categoricas),\n",
    "        ('bin', binary_transformer, columnas_binarias)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código define una función llamada evaluar_modelo, que evalúa el rendimiento de un modelo de regresión. Toma tres parámetros: y_true (valores reales), y_pred (valores predichos) y nombre_modelo (nombre del modelo para mostrar en los resultados). La función calcula tres métricas de evaluación comunes: MAE (Error absoluto medio), RMSE (Raíz del error cuadrático medio) y R2 (Coeficiente de determinación). Luego, imprime estos resultados y devuelve un diccionario con los valores de estas métricas para su posterior análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definir función de evaluación\n",
    "def evaluar_modelo(y_true, y_pred, nombre_modelo):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nResultados para {nombre_modelo}:\")\n",
    "    print(f\"MAE: {mae:,.2f} €\")\n",
    "    print(f\"RMSE: {rmse:,.2f} €\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código realiza una serie de pasos para procesar y entrenar un modelo de regresión Random Forest en un conjunto de datos. Primero, analiza la distribución de muestras por barrio y agrupa aquellos con menos de 5 muestras en una categoría \"OTROS\". Luego, divide los datos en conjuntos de entrenamiento y prueba, asegurándose de que ambas particiones contengan las mismas categorías de variables categóricas.\n",
    "\n",
    "A continuación, transforma los datos utilizando un preprocesador (que incluye normalización de variables numéricas, One-Hot Encoding de variables categóricas y conversión de variables binarias), y entrena un modelo Random Forest usando una búsqueda en cuadrícula (GridSearchCV) para optimizar los hiperparámetros como el número de estimadores, la profundidad máxima y el número mínimo de muestras para dividir.\n",
    "\n",
    "Finalmente, se entrena el modelo con los mejores parámetros encontrados, se realiza la predicción sobre el conjunto de prueba y se evalúa el rendimiento del modelo utilizando métricas como MAE, RMSE y R² mediante la función evaluar_modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de muestras por barrio:\n",
      "neighborhood\n",
      "22    160\n",
      "24    144\n",
      "23    143\n",
      "30    133\n",
      "89    124\n",
      "     ... \n",
      "21      3\n",
      "79      2\n",
      "65      1\n",
      "55      1\n",
      "11      1\n",
      "Name: count, Length: 114, dtype: Int64\n",
      "\n",
      "Barrios con pocas muestras (<=5):\n",
      "neighborhood\n",
      "134    5\n",
      "80     5\n",
      "63     5\n",
      "48     4\n",
      "21     3\n",
      "79     2\n",
      "65     1\n",
      "55     1\n",
      "11     1\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Verificando categorías únicas por variable categórica:\n",
      "\n",
      "neighborhood:\n",
      "Categorías solo en test: {'65'}\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\TFG-IDEALISTA\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:202: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejores parámetros para Random Forest: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Resultados para Random Forest:\n",
      "MAE: 10,509.34 €\n",
      "RMSE: 55,545.44 €\n",
      "R2 Score: 0.9884\n"
     ]
    }
   ],
   "source": [
    "# 1. Analizar la distribución de muestras por barrio\n",
    "print(\"Distribución de muestras por barrio:\")\n",
    "neighborhood_counts = X['neighborhood'].value_counts()\n",
    "print(neighborhood_counts)\n",
    "\n",
    "print(\"\\nBarrios con pocas muestras (<=5):\")\n",
    "print(neighborhood_counts[neighborhood_counts <= 5])\n",
    "\n",
    "# 2. Usar train_test_split normal pero con shuffle\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 3. Verificar la distribución en train y test\n",
    "print(\"\\nVerificando categorías únicas por variable categórica:\")\n",
    "for col in columnas_categoricas:\n",
    "    train_categories = set(X_train[col].unique())\n",
    "    test_categories = set(X_test[col].unique())\n",
    "    diff_categories = test_categories - train_categories\n",
    "    if len(diff_categories) > 0:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"Categorías solo en test: {diff_categories}\")\n",
    "\n",
    "# 4. Continuar con el preprocesamiento y entrenamiento\n",
    "preprocessor.fit(X_train)\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# 5. Configurar y entrenar Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_model, \n",
    "    param_grid=rf_param_grid, \n",
    "    cv=3, \n",
    "    n_jobs=1,  # Disable parallel processing to avoid pickling issues\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 6. Entrenar el modelo\n",
    "rf_grid_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# 7. Evaluar el modelo\n",
    "print(f\"\\nMejores parámetros para Random Forest: {rf_grid_search.best_params_}\")\n",
    "y_pred = rf_grid_search.predict(X_test_transformed)\n",
    "resultados = evaluar_modelo(y_test, y_pred, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Entrenar múltiples modelos para comparar\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_score\n",
    "\n",
    "\n",
    "def entrenar_y_evaluar_modelos(X_train, X_test, y_train, y_test):\n",
    "    modelos = {\n",
    "        'Random Forest': RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=2,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=7,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "\n",
    "     # Parámetros para búsqueda aleatoria de Random Forest\n",
    "    param_dist_rf = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Parámetros para búsqueda aleatoria de XGBoost\n",
    "    param_dist_xgb = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.7, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    resultados = {}\n",
    "\n",
    "    for nombre, modelo in modelos.items():\n",
    "        print(f\"\\nEntrenando {nombre}...\")\n",
    "\n",
    "        # Dependiendo del modelo, definimos la búsqueda aleatoria\n",
    "        if nombre == 'Random Forest':\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=modelo,\n",
    "                param_distributions=param_dist_rf,\n",
    "                n_iter=10,\n",
    "                cv=5,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        elif nombre == 'XGBoost':\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=modelo,\n",
    "                param_distributions=param_dist_xgb,\n",
    "                n_iter=10,\n",
    "                cv=5,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "             # Entrenar el modelo con la búsqueda aleatoria\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        # Obtener el mejor modelo de la búsqueda aleatoria\n",
    "        best_model = random_search.best_estimator_\n",
    "\n",
    "        # Validación cruzada para evaluar el modelo\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(best_model, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
    "\n",
    "        print(f\"\\n{nombre} - Mejor modelo: {random_search.best_params_}\")\n",
    "        print(f\"{nombre} - MAE promedio de validación cruzada: {-scores.mean()}\")\n",
    "\n",
    "        # Entrenar el modelo con los mejores parámetros y predecir sobre el conjunto de test\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluar el modelo final en el conjunto de test\n",
    "        resultados[nombre] = evaluar_modelo(y_test, y_pred, nombre)\n",
    "\n",
    "    return modelos, resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando Random Forest...\n",
      "\n",
      "Random Forest - Mejor modelo: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': None}\n",
      "Random Forest - MAE promedio de validación cruzada: 13081.342889575306\n",
      "\n",
      "Resultados para Random Forest:\n",
      "MAE: 10,237.11 €\n",
      "RMSE: 54,325.80 €\n",
      "R2 Score: 0.9889\n",
      "\n",
      "Entrenando XGBoost...\n",
      "\n",
      "XGBoost - Mejor modelo: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "XGBoost - MAE promedio de validación cruzada: 11462.812168423332\n",
      "\n",
      "Resultados para XGBoost:\n",
      "MAE: 9,333.08 €\n",
      "RMSE: 37,060.43 €\n",
      "R2 Score: 0.9948\n"
     ]
    }
   ],
   "source": [
    "# 2. Analizar importancia de características\n",
    "def analizar_importancia_features(modelo, feature_names):\n",
    "    if hasattr(modelo, 'feature_importances_'):\n",
    "        importances = modelo.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        print(\"\\nImportancia de características:\")\n",
    "        for f in range(len(feature_names)):\n",
    "            print(\"%d. %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))\n",
    "\n",
    "\n",
    "# 3. Obtener nombres de características después del preprocesamiento\n",
    "def get_feature_names(preprocessor, columnas_numericas, columnas_categoricas, columnas_binarias):\n",
    "    numeric_features = columnas_numericas\n",
    "    categorical_features = []\n",
    "    for i, col in enumerate(columnas_categoricas):\n",
    "        cats = preprocessor.named_transformers_['cat'].named_steps['onehot'].categories_[i][1:]\n",
    "        categorical_features.extend([f\"{col}_{cat}\" for cat in cats])\n",
    "    binary_features = columnas_binarias\n",
    "    return numeric_features + categorical_features + binary_features\n",
    "\n",
    "# 4. Entrenar y evaluar modelos\n",
    "modelos, resultados = entrenar_y_evaluar_modelos(X_train_transformed, X_test_transformed, y_train, y_test)\n",
    "\n",
    "# 5. Obtener nombres de características y analizar importancia\n",
    "feature_names = get_feature_names(preprocessor, columnas_numericas, columnas_categoricas, columnas_binarias)\n",
    "analizar_importancia_features(modelos['XGBoost'], feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Random Forest, R2 Score: 0.9889\n",
      "Modelo: XGBoost, R2 Score: 0.9948\n",
      "\n",
      "Mejor modelo: XGBoost con R2 Score: 0.9948\n",
      "Modelo guardado en: ../data/models/mejor_modelo.joblib\n",
      "Preprocessor guardado en: ../data/models/preprocessor.joblib\n"
     ]
    }
   ],
   "source": [
    "def guardar_mejor_modelo(modelos, resultados, ruta_modelo='../data/models/mejor_modelo.joblib', ruta_preprocessor='../data/models/preprocessor.joblib'):\n",
    "    \"\"\"\n",
    "    Compara los modelos basándose en R2 score y guarda el mejor.\n",
    "    \n",
    "    Args:\n",
    "        modelos (dict): Diccionario con los modelos entrenados\n",
    "        resultados (dict): Diccionario con las métricas de cada modelo\n",
    "        ruta_modelo (str): Ruta donde guardar el mejor modelo\n",
    "        ruta_preprocessor (str): Ruta donde guardar el preprocessor\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (nombre_mejor_modelo, r2_score)\n",
    "    \"\"\"\n",
    "    # Encontrar el modelo con mejor R2 score\n",
    "    mejor_r2 = -float('inf')\n",
    "    mejor_modelo_nombre = None\n",
    "    \n",
    "    for nombre, metricas in resultados.items():\n",
    "        r2_actual = metricas['r2']\n",
    "        print(f\"Modelo: {nombre}, R2 Score: {r2_actual:.4f}\")\n",
    "        \n",
    "        if r2_actual > mejor_r2:\n",
    "            mejor_r2 = r2_actual\n",
    "            mejor_modelo_nombre = nombre\n",
    "    \n",
    "    print(f\"\\nMejor modelo: {mejor_modelo_nombre} con R2 Score: {mejor_r2:.4f}\")\n",
    "    \n",
    "    # Guardar el mejor modelo y el preprocessor\n",
    "    mejor_modelo = modelos[mejor_modelo_nombre]\n",
    "    joblib.dump(mejor_modelo, ruta_modelo)\n",
    "    joblib.dump(preprocessor, ruta_preprocessor)\n",
    "    \n",
    "    print(f\"Modelo guardado en: {ruta_modelo}\")\n",
    "    print(f\"Preprocessor guardado en: {ruta_preprocessor}\")\n",
    "    \n",
    "    return mejor_modelo_nombre, mejor_r2\n",
    "\n",
    "# Uso de la función:\n",
    "mejor_modelo_nombre, mejor_r2 = guardar_mejor_modelo(modelos, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelos definidos:\n",
      "XGBoost: <class 'xgboost.sklearn.XGBRegressor'>\n",
      "Random Forest: <class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
      "\n",
      "Mejor modelo identificado: XGBoost\n",
      "Mejor R2 Score: 0.9948\n"
     ]
    }
   ],
   "source": [
    "# === DEFINIR MODELOS PARA ENTRENAMIENTO FINAL ===\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definir modelos con los mejores parámetros encontrados\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Modelos definidos:\")\n",
    "print(f\"XGBoost: {type(xgb_model)}\")\n",
    "print(f\"Random Forest: {type(rf_model)}\")\n",
    "\n",
    "# Verificar que el mejor modelo está identificado\n",
    "print(f\"\\nMejor modelo identificado: {mejor_modelo_nombre}\")\n",
    "print(f\"Mejor R2 Score: {mejor_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICACIÓN DEL MODELO ===\n",
      "\n",
      "Modelo: Random Forest\n",
      "Tipo: <class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
      "¿Tiene método predict?: True\n",
      "N estimators: 200\n",
      "❌ Error en predicción: This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "\n",
      "Modelo: XGBoost\n",
      "Tipo: <class 'xgboost.sklearn.XGBRegressor'>\n",
      "¿Tiene método predict?: True\n",
      "N estimators: 200\n",
      "❌ Error en predicción: need to call fit or load_model beforehand\n",
      "\n",
      "=== ENTRENAMIENTO DEL MEJOR MODELO ===\n",
      "Mejor modelo seleccionado: XGBoost\n",
      "Tipo del mejor modelo: <class 'xgboost.sklearn.XGBRegressor'>\n",
      "Entrenando el modelo...\n",
      "✅ Modelo entrenado exitosamente\n",
      "✅ Predicción de prueba exitosa\n",
      "\n",
      "=== GUARDANDO MODELO ===\n",
      "R2 Score del mejor modelo: 0.9948\n",
      "Guardando modelo en: c:\\Users\\HP\\Desktop\\tfg-alvaro-carrera-idealista\\backend\\data\\models\\mejor_modelo.joblib\n",
      "Guardando preprocessor en: c:\\Users\\HP\\Desktop\\tfg-alvaro-carrera-idealista\\backend\\data\\models\\preprocessor.joblib\n",
      "✅ Modelo y preprocessor guardados correctamente\n",
      "\n",
      "=== VERIFICACIÓN DE CARGA ===\n",
      "Modelo cargado - Tipo: <class 'xgboost.sklearn.XGBRegressor'>\n",
      "¿Tiene método predict?: True\n",
      "✅ Predicción con modelo cargado exitosa\n",
      "Predicción de prueba: 180958.38\n",
      "\n",
      "=== RESUMEN ===\n",
      "Mejor modelo: XGBoost\n",
      "R2 Score: 0.9948\n",
      "El modelo está listo para usar en el backend\n"
     ]
    }
   ],
   "source": [
    "# === VERIFICACIÓN Y ENTRENAMIENTO FINAL ===\n",
    "print(\"=== VERIFICACIÓN DEL MODELO ===\\n\")\n",
    "\n",
    "# Verificar estado de los modelos definidos anteriormente\n",
    "for nombre, modelo in [(\"Random Forest\", rf_model), (\"XGBoost\", xgb_model)]:\n",
    "    print(f\"Modelo: {nombre}\")\n",
    "    print(f\"Tipo: {type(modelo)}\")\n",
    "    print(f\"¿Tiene método predict?: {hasattr(modelo, 'predict')}\")\n",
    "    \n",
    "    if hasattr(modelo, 'n_estimators'):\n",
    "        print(f\"N estimators: {modelo.n_estimators}\")\n",
    "    \n",
    "    # Probar predicción\n",
    "    try:\n",
    "        test_pred = modelo.predict(X_test_transformed[:1])\n",
    "        print(\"✅ Predicción exitosa\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en predicción: {e}\")\n",
    "    print()\n",
    "\n",
    "# Encontrar el mejor modelo\n",
    "print(\"=== ENTRENAMIENTO DEL MEJOR MODELO ===\")\n",
    "\n",
    "# Determinar cuál fue el mejor modelo\n",
    "if 'XGBoost' in mejor_modelo_nombre:\n",
    "    mejor_modelo = xgb_model\n",
    "    print(\"Mejor modelo seleccionado: XGBoost\")\n",
    "else:\n",
    "    mejor_modelo = rf_model\n",
    "    print(\"Mejor modelo seleccionado: Random Forest\")\n",
    "\n",
    "print(f\"Tipo del mejor modelo: {type(mejor_modelo)}\")\n",
    "\n",
    "# Entrenar el modelo con todos los datos\n",
    "print(\"Entrenando el modelo...\")\n",
    "mejor_modelo.fit(X_train_transformed, y_train)\n",
    "print(\"✅ Modelo entrenado exitosamente\")\n",
    "\n",
    "# Verificar que funciona después del entrenamiento\n",
    "try:\n",
    "    test_pred = mejor_modelo.predict(X_test_transformed[:1])\n",
    "    print(\"✅ Predicción de prueba exitosa\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error en predicción después del entrenamiento: {e}\")\n",
    "\n",
    "print(f\"\\n=== GUARDANDO MODELO ===\")\n",
    "print(f\"R2 Score del mejor modelo: {mejor_r2:.4f}\")\n",
    "\n",
    "# Paths absolutos\n",
    "import os\n",
    "base_path = r\"c:\\Users\\HP\\Desktop\\tfg-alvaro-carrera-idealista\\backend\"\n",
    "model_path = os.path.join(base_path, \"data\", \"models\", \"mejor_modelo.joblib\")\n",
    "preprocessor_path = os.path.join(base_path, \"data\", \"models\", \"preprocessor.joblib\")\n",
    "\n",
    "print(f\"Guardando modelo en: {model_path}\")\n",
    "print(f\"Guardando preprocessor en: {preprocessor_path}\")\n",
    "\n",
    "# Guardar\n",
    "joblib.dump(mejor_modelo, model_path)\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(\"✅ Modelo y preprocessor guardados correctamente\")\n",
    "\n",
    "print(f\"\\n=== VERIFICACIÓN DE CARGA ===\")\n",
    "# Cargar para verificar\n",
    "modelo_cargado = joblib.load(model_path)\n",
    "preprocessor_cargado = joblib.load(preprocessor_path)\n",
    "\n",
    "print(f\"Modelo cargado - Tipo: {type(modelo_cargado)}\")\n",
    "print(f\"¿Tiene método predict?: {hasattr(modelo_cargado, 'predict')}\")\n",
    "\n",
    "# Probar predicción con modelo cargado\n",
    "try:\n",
    "    pred_test = modelo_cargado.predict(X_test_transformed[:1])\n",
    "    print(\"✅ Predicción con modelo cargado exitosa\")\n",
    "    print(f\"Predicción de prueba: {pred_test[0]:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al cargar modelo: {e}\")\n",
    "\n",
    "print(f\"\\n=== RESUMEN ===\")\n",
    "print(f\"Mejor modelo: {mejor_modelo_nombre}\")\n",
    "print(f\"R2 Score: {mejor_r2:.4f}\")\n",
    "print(f\"El modelo está listo para usar en el backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RECREANDO PREPROCESSOR ===\n",
      "Columnas numéricas: 9\n",
      "Columnas binarias: 16\n",
      "Columnas categóricas: 4\n",
      "✅ Todas las columnas están disponibles\n",
      "✅ Preprocessor estándar creado\n",
      "Entrenando preprocessor...\n",
      "✅ Preprocessor entrenado\n",
      "Forma de datos transformados - Train: (5388, 167)\n",
      "Forma de datos transformados - Test: (1347, 167)\n",
      "Verificando compatibilidad...\n",
      "Muestra de prueba transformada: (1, 167)\n",
      "✅ Preprocessor funcionando correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\TFG-IDEALISTA\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:202: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === RECREAR PREPROCESSOR CON TRANSFORMADORES ESTÁNDAR ===\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"=== RECREANDO PREPROCESSOR ===\")\n",
    "\n",
    "# Definir las columnas usando los nombres reales del dataset\n",
    "columnas_numericas = ['sq_mt_built', 'sq_mt_useful', 'n_rooms', 'n_bathrooms', \n",
    "                     'floor', 'built_year', 'buy_price_by_area', 'latitude', 'longitude']\n",
    "columnas_binarias = ['has_lift', 'is_exterior', 'has_parking', 'is_new_development',\n",
    "                    'has_central_heating', 'has_individual_heating', 'has_ac',\n",
    "                    'has_garden', 'has_pool', 'has_terrace', 'has_storage_room',\n",
    "                    'is_furnished', 'is_orientation_north', 'is_orientation_south',\n",
    "                    'is_orientation_east', 'is_orientation_west']\n",
    "columnas_categoricas = ['house_type', 'energy_certificate', 'district', 'neighborhood']\n",
    "\n",
    "print(f\"Columnas numéricas: {len(columnas_numericas)}\")\n",
    "print(f\"Columnas binarias: {len(columnas_binarias)}\")\n",
    "print(f\"Columnas categóricas: {len(columnas_categoricas)}\")\n",
    "\n",
    "# Verificar que todas las columnas existen\n",
    "todas_columnas = columnas_numericas + columnas_binarias + columnas_categoricas\n",
    "columnas_disponibles = set(X_train.columns)\n",
    "columnas_faltantes = set(todas_columnas) - columnas_disponibles\n",
    "if columnas_faltantes:\n",
    "    print(f\"⚠️ Columnas faltantes: {columnas_faltantes}\")\n",
    "else:\n",
    "    print(\"✅ Todas las columnas están disponibles\")\n",
    "\n",
    "# Transformadores usando solo funciones estándar de sklearn\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Para las columnas binarias, simplemente las dejamos como están (ya son 0/1)\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('passthrough', 'passthrough')  # No transformación\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Crear el preprocessor estándar\n",
    "preprocessor_standard = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, columnas_numericas),\n",
    "        ('bin', binary_transformer, columnas_binarias),\n",
    "        ('cat', categorical_transformer, columnas_categoricas)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"✅ Preprocessor estándar creado\")\n",
    "\n",
    "# Entrenar el preprocessor con los datos de entrenamiento\n",
    "print(\"Entrenando preprocessor...\")\n",
    "preprocessor_standard.fit(X_train)\n",
    "print(\"✅ Preprocessor entrenado\")\n",
    "\n",
    "# Transformar los datos\n",
    "X_train_transformed_new = preprocessor_standard.transform(X_train)\n",
    "X_test_transformed_new = preprocessor_standard.transform(X_test)\n",
    "\n",
    "print(f\"Forma de datos transformados - Train: {X_train_transformed_new.shape}\")\n",
    "print(f\"Forma de datos transformados - Test: {X_test_transformed_new.shape}\")\n",
    "\n",
    "# Verificar que funciona\n",
    "print(\"Verificando compatibilidad...\")\n",
    "test_sample = X_train.iloc[:1]\n",
    "test_transformed = preprocessor_standard.transform(test_sample)\n",
    "print(f\"Muestra de prueba transformada: {test_transformed.shape}\")\n",
    "print(\"✅ Preprocessor funcionando correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICACIÓN DE DATOS ===\n",
      "Forma de X_train: (5388, 29)\n",
      "Columnas disponibles en X_train:\n",
      "['sq_mt_built', 'sq_mt_useful', 'n_rooms', 'n_bathrooms', 'floor', 'built_year', 'buy_price_by_area', 'latitude', 'longitude', 'house_type', 'energy_certificate', 'district', 'neighborhood', 'has_lift', 'is_exterior', 'has_parking', 'is_new_development', 'has_central_heating', 'has_individual_heating', 'has_ac', 'has_garden', 'has_pool', 'has_terrace', 'has_storage_room', 'is_furnished', 'is_orientation_north', 'is_orientation_south', 'is_orientation_east', 'is_orientation_west']\n",
      "\n",
      "Tipo de X_train: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "Forma de y_train: (5388,)\n",
      "Tipo de y_train: <class 'pandas.core.series.Series'>\n",
      "\n",
      "Primeras 3 filas de X_train:\n",
      "      sq_mt_built  sq_mt_useful  n_rooms  n_bathrooms  floor  built_year  \\\n",
      "4935         65.0     94.340591      3.0          1.0    3.0      1960.0   \n",
      "349          85.0     75.000000      3.0          2.0    1.0      1970.0   \n",
      "2183         75.0     94.340591      3.0          1.0    5.0      1970.0   \n",
      "\n",
      "      buy_price_by_area   latitude  longitude house_type  ... has_ac  \\\n",
      "4935             2246.0  40.388455  -3.731310          1  ...    0.0   \n",
      "349              3459.0  40.373015  -3.612061          1  ...    1.0   \n",
      "2183             3053.0  40.397049  -3.762057          1  ...    0.0   \n",
      "\n",
      "     has_garden has_pool  has_terrace  has_storage_room  is_furnished  \\\n",
      "4935        0.0      0.0          0.0               0.0           0.0   \n",
      "349         0.0      1.0          0.0               1.0           0.0   \n",
      "2183        0.0      0.0          0.0               0.0           0.0   \n",
      "\n",
      "      is_orientation_north  is_orientation_south  is_orientation_east  \\\n",
      "4935                   0.0                   0.0                  0.0   \n",
      "349                    0.0                   0.0                  0.0   \n",
      "2183                   0.0                   0.0                  0.0   \n",
      "\n",
      "      is_orientation_west  \n",
      "4935                  0.0  \n",
      "349                   0.0  \n",
      "2183                  0.0  \n",
      "\n",
      "[3 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# === VERIFICAR ESTRUCTURA DE DATOS ===\n",
    "print(\"=== VERIFICACIÓN DE DATOS ===\")\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Columnas disponibles en X_train:\")\n",
    "print(list(X_train.columns))\n",
    "print(f\"\\nTipo de X_train: {type(X_train)}\")\n",
    "\n",
    "# También verificar y_train\n",
    "print(f\"\\nForma de y_train: {y_train.shape}\")\n",
    "print(f\"Tipo de y_train: {type(y_train)}\")\n",
    "\n",
    "# Mostrar una muestra de los datos\n",
    "print(f\"\\nPrimeras 3 filas de X_train:\")\n",
    "print(X_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENTRENAMIENTO CON PREPROCESSOR ESTÁNDAR ===\n",
      "Datos transformados - Train: (5388, 167)\n",
      "Datos transformados - Test: (1347, 167)\n",
      "Entrenando modelo XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\TFG-IDEALISTA\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:202: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo entrenado\n",
      "R2 Score con preprocessor estándar: 0.9938\n",
      "MSE: 1642576997.04\n",
      "\n",
      "=== GUARDANDO MODELO ESTÁNDAR ===\n",
      "Guardando modelo en: c:\\Users\\HP\\Desktop\\tfg-alvaro-carrera-idealista\\backend\\data\\models\\mejor_modelo.joblib\n",
      "Guardando preprocessor en: c:\\Users\\HP\\Desktop\\tfg-alvaro-carrera-idealista\\backend\\data\\models\\preprocessor.joblib\n",
      "✅ Modelo y preprocessor guardados\n",
      "\n",
      "=== VERIFICACIÓN DE CARGA ===\n",
      "Modelo cargado - Tipo: <class 'xgboost.sklearn.XGBRegressor'>\n",
      "Preprocessor cargado - Tipo: <class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "✅ Predicción exitosa: 180958.38\n",
      "🎉 ¡El modelo está listo para producción!\n"
     ]
    }
   ],
   "source": [
    "# === ENTRENAR MODELO CON NUEVO PREPROCESSOR ===\n",
    "print(\"=== ENTRENAMIENTO CON PREPROCESSOR ESTÁNDAR ===\")\n",
    "\n",
    "# Usar el preprocessor estándar que acabamos de crear\n",
    "X_train_std = preprocessor_standard.transform(X_train)\n",
    "X_test_std = preprocessor_standard.transform(X_test)\n",
    "\n",
    "print(f\"Datos transformados - Train: {X_train_std.shape}\")\n",
    "print(f\"Datos transformados - Test: {X_test_std.shape}\")\n",
    "\n",
    "# Crear un nuevo modelo XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "xgb_final = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Entrenando modelo XGBoost...\")\n",
    "xgb_final.fit(X_train_std, y_train)\n",
    "print(\"✅ Modelo entrenado\")\n",
    "\n",
    "# Evaluar el modelo\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "y_pred_std = xgb_final.predict(X_test_std)\n",
    "r2_std = r2_score(y_test, y_pred_std)\n",
    "mse_std = mean_squared_error(y_test, y_pred_std)\n",
    "\n",
    "print(f\"R2 Score con preprocessor estándar: {r2_std:.4f}\")\n",
    "print(f\"MSE: {mse_std:.2f}\")\n",
    "\n",
    "# Guardar el modelo y preprocessor estándar\n",
    "import os\n",
    "base_path = r\"c:\\Users\\HP\\Desktop\\tfg-alvaro-carrera-idealista\\backend\"\n",
    "model_path = os.path.join(base_path, \"data\", \"models\", \"mejor_modelo.joblib\")\n",
    "preprocessor_path = os.path.join(base_path, \"data\", \"models\", \"preprocessor.joblib\")\n",
    "\n",
    "print(f\"\\n=== GUARDANDO MODELO ESTÁNDAR ===\")\n",
    "print(f\"Guardando modelo en: {model_path}\")\n",
    "print(f\"Guardando preprocessor en: {preprocessor_path}\")\n",
    "\n",
    "joblib.dump(xgb_final, model_path)\n",
    "joblib.dump(preprocessor_standard, preprocessor_path)\n",
    "print(\"✅ Modelo y preprocessor guardados\")\n",
    "\n",
    "# Verificar carga\n",
    "print(f\"\\n=== VERIFICACIÓN DE CARGA ===\")\n",
    "modelo_cargado = joblib.load(model_path)\n",
    "preprocessor_cargado = joblib.load(preprocessor_path)\n",
    "\n",
    "print(f\"Modelo cargado - Tipo: {type(modelo_cargado)}\")\n",
    "print(f\"Preprocessor cargado - Tipo: {type(preprocessor_cargado)}\")\n",
    "\n",
    "# Probar predicción\n",
    "test_sample = X_test.iloc[:1]\n",
    "test_transformed = preprocessor_cargado.transform(test_sample)\n",
    "pred_test = modelo_cargado.predict(test_transformed)\n",
    "\n",
    "print(f\"✅ Predicción exitosa: {pred_test[0]:.2f}\")\n",
    "print(\"🎉 ¡El modelo está listo para producción!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG-IDEALISTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
